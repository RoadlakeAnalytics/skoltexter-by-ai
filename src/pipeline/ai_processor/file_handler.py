"""File persistence utilities for AI processing output.

This module is responsible exclusively for local file I/O: discovering markdown files to be processed
and saving both AI-processed outputs and raw JSON responses. It does not interact with external services
or APIs. It is architecturally decoupled from business logic and should be invoked only by upper pipeline
layers to persist or enumerate files as needed.

This file upholds the single-responsibility principle: all coordination, error handling, and logging
for optional artifact persistence are performed here. All configuration is sourced via ``src/config.py``.
No global state is mutated, and all errors are either logged defensively or propagated per policy.

"""

import json
import logging
from pathlib import Path
from typing import Any

from src.config import (
    AI_PROCESSED_FILENAME_SUFFIX,
    AI_PROCESSED_MARKDOWN_SUBDIR,
    AI_RAW_RESPONSE_FILENAME_SUFFIX,
    AI_RAW_RESPONSES_SUBDIR,
)


def find_markdown_files(input_dir: Path) -> list[Path]:
    """Return all Markdown files found in the given input directory.

    Searches the provided directory for files with the ``.md`` extension
    and returns their absolute Path objects, sorted lexicographically.
    Designed as an isolated, side-effect-free scanner for downstream pipelines.

    Parameters
    ----------
    input_dir : Path
        Directory in which to search for files with the ``.md`` suffix.

    Returns
    -------
    files : list of Path
        Sorted list of all markdown paths found in `input_dir`.
        The list may be empty if no such files exist.

    Raises
    ------
    None.
        This function never raises (it will propagate unexpected
        OS or permissions errors per standard library semantics).

    Notes
    -----
    This function never recurses into subdirectories.
    Path matching is case-sensitive and performed using the standard library ``Path.glob``.

    Examples
    --------
    >>> from pathlib import Path
    >>> from src.pipeline.ai_processor.file_handler import find_markdown_files
    >>> import tempfile
    >>> d = tempfile.TemporaryDirectory()
    >>> p = Path(d.name)
    >>> f1 = p / "a.md"; f2 = p / "b.md"; f1.write_text("#1"); f2.write_text("#2")
    >>> sorted(find_markdown_files(p)) == sorted([f1, f2])
    True
    >>> d.cleanup()
    """
    return sorted(list(Path(input_dir).glob("*.md")))


def save_processed_files(
    school_id: str, markdown: str, raw_json: dict[str, Any], output_dir_base: Path
) -> None:
    """Persist both AI-processed markdown and the raw JSON response for a given school.

    This routine writes two output artifacts:
    (1) The plain Markdown file as generated by the AI, and
    (2) The original JSON response from the AI client.
    All directories are created as needed beneath ``output_dir_base``,
    using names and suffixes from configuration.

    All file I/O exceptions are caught and logged, never raised. This ensures
    that persistence failures (e.g. permission errors, disk full, monkeypatched
    libraries during testing) cannot break upstream business logic. Loss of these
    artifacts is not considered fatal to pipeline processing.

    Parameters
    ----------
    school_id : str
        School identifier string for filename prefix (usually unique).
    markdown : str
        The Markdown text to be written as output.
    raw_json : dict of (str, Any)
        Raw JSON dictionary response as received from the AI client.
    output_dir_base : Path
        Base directory under which all output folders are created.

    Returns
    -------
    None.
        This function is called for side effects only.

    Raises
    ------
    None.
        All exceptions from file writing are caught internally and logged.

    Notes
    -----
    Subdirectory and file name conventions are controlled through constants,
    and may be changed via ``src/config.py`` only. Logging is performed using the
    module-level logger. If both writes fail, both errors are independently logged.

    Examples
    --------
    >>> from pathlib import Path
    >>> import tempfile
    >>> from src.pipeline.ai_processor.file_handler import save_processed_files
    >>> d = tempfile.TemporaryDirectory()
    >>> base = Path(d.name)
    >>> save_processed_files("test", "# Example", {"field": 1}, base)
    >>> (base / "ai_markdown" / "test.ai.md").exists()
    True
    >>> (base / "ai_raw" / "test.ai.json").exists()
    True
    >>> d.cleanup()
    """
    md_dir = output_dir_base / AI_PROCESSED_MARKDOWN_SUBDIR
    json_dir = output_dir_base / AI_RAW_RESPONSES_SUBDIR
    md_dir.mkdir(parents=True, exist_ok=True)
    json_dir.mkdir(parents=True, exist_ok=True)
    logger = logging.getLogger(__name__)

    md_path = md_dir / f"{school_id}{AI_PROCESSED_FILENAME_SUFFIX}"
    json_path = json_dir / f"{school_id}{AI_RAW_RESPONSE_FILENAME_SUFFIX}"

    # Attempt writes but do not raise on failure; callers should not be
    # disrupted by I/O issues when persisting optional artifacts.
    try:
        md_path.write_text(markdown, encoding="utf-8")
    except Exception as exc:  # pragma: no cover - defensive logging path
        # Be defensive: tests sometimes monkeypatch I/O helpers and may raise
        # arbitrary exceptions. We do not want optional persistence failures
        # to crash the processing pipeline.
        logger.exception(
            "Failed to write processed markdown for %s: %s", school_id, exc
        )

    try:
        json_path.write_text(
            json.dumps(raw_json, ensure_ascii=False, indent=2), encoding="utf-8"
        )
    except Exception as exc:  # pragma: no cover - defensive logging path
        logger.exception("Failed to write raw JSON response for %s: %s", school_id, exc)
